{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {},
  "cells": [
    {
      "id": "7c67918d",
      "cell_type": "markdown",
      "source": "# Face Recognition (AT&T / ORL) ‚Äî 2 Pipelines in Google Colab\n\n**Pipeline A (Baseline):** PCA (‚ÄúEigenfaces-style‚Äù) + Linear SVM  \n**Pipeline B (Winner):** Pretrained CNN embeddings (ResNet18, feature extractor only) + Linear SVM  \n\n‚úÖ Offline evaluation only (accuracy, macro P/R/F1, confusion matrix, runtime)  \n‚úÖ Same identity-stratified split for both pipelines (7 train / 3 test per person)  \n‚úÖ Extra comparison graphs (per-seed + aggregate)\n\n> Run the notebook **top ‚Üí bottom** on a fresh runtime.\n",
      "metadata": {}
    },
    {
      "id": "988ddcb3",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "# CELL 0 ‚Äî Setup + Settings + Utilities\n\nimport os, sys, time, zipfile, shutil, math\nfrom pathlib import Path\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\n# -----------------------------\n# Experiment settings\n# -----------------------------\nSEEDS = [42, 123, 256, 789, 1024]\nN_TEST_PER_SUBJECT = 3      # ORL has 10 images/subject -> 7 train / 3 test\nAPPLY_HIST_EQ = True        # histogram equalization on grayscale\n\nRESULTS_DIR = Path(\"/content/results\")\nRESULTS_DIR.mkdir(parents=True, exist_ok=True)\n\n# Pipeline A grids\nPCA_COMPONENTS_GRID = [20, 40, 60, 80, 100, 120, 150]\nSVM_C_GRID = [0.1, 1, 10, 100]\n\n# Embedding extraction\nEMB_BATCH_SIZE = 64\n\n# -----------------------------\n# Helpers\n# -----------------------------\ndef eval_metrics(y_true, y_pred):\n    acc = accuracy_score(y_true, y_pred)\n    p, r, f1, _ = precision_recall_fscore_support(\n        y_true, y_pred, average=\"macro\", zero_division=0\n    )\n    return acc, p, r, f1\n\ndef per_class_accuracy(cm):\n    # cm: rows=true, cols=pred\n    denom = cm.sum(axis=1)\n    denom = np.maximum(denom, 1)\n    return np.diag(cm) / denom\n\ndef save_confusion(y_true, y_pred, title, outpath: Path, n_classes):\n    labels = list(range(n_classes))\n    cm = confusion_matrix(y_true, y_pred, labels=labels)\n\n    fig, ax = plt.subplots(figsize=(10, 10))\n    disp = ConfusionMatrixDisplay(\n        confusion_matrix=cm,\n        display_labels=[f\"s{i+1}\" for i in labels]\n    )\n    disp.plot(ax=ax, cmap=\"Blues\", colorbar=False, xticks_rotation=\"vertical\")\n    ax.set_title(title)\n    ax.tick_params(axis=\"x\", labelsize=6)\n    ax.tick_params(axis=\"y\", labelsize=6)\n    plt.tight_layout()\n    fig.savefig(outpath, dpi=200)\n    plt.show()\n    return cm\n\ndef show_grid(X, y, n=20, cols=10, seed=0):\n    rng = np.random.default_rng(seed)\n    idx = rng.choice(len(X), size=min(n, len(X)), replace=False)\n    rows = math.ceil(len(idx) / cols)\n    plt.figure(figsize=(cols*1.2, rows*1.2))\n    for i, k in enumerate(idx, 1):\n        plt.subplot(rows, cols, i)\n        plt.imshow(X[k], cmap=\"gray\")\n        plt.title(f\"s{y[k]+1}\", fontsize=8)\n        plt.axis(\"off\")\n    plt.tight_layout()\n    plt.show()\n\ndef split_per_identity(y, seed: int, n_test_per_subject: int):\n    rng = np.random.default_rng(seed)\n    y = np.asarray(y)\n    train_idx, test_idx = [], []\n    for cls in np.unique(y):\n        idx = np.where(y == cls)[0].copy()\n        rng.shuffle(idx)\n        test = idx[:n_test_per_subject]\n        train = idx[n_test_per_subject:]\n        train_idx.extend(train.tolist())\n        test_idx.extend(test.tolist())\n    return np.array(sorted(train_idx), dtype=np.int64), np.array(sorted(test_idx), dtype=np.int64)\n\ndef find_att_root(search_root: Path):\n    # Find a folder that contains s1..s40 directories with .pgm files inside.\n    candidates = [search_root] + [p for p in search_root.rglob(\"*\") if p.is_dir()]\n    for root in candidates:\n        try:\n            s_dirs = [p for p in root.iterdir() if p.is_dir() and p.name.startswith(\"s\")]\n        except Exception:\n            continue\n        if len(s_dirs) < 40:\n            continue\n        s1 = root / \"s1\"\n        if not s1.exists():\n            continue\n        if len(list(s1.glob(\"*.pgm\"))) == 0:\n            continue\n        return root\n    return None\n\nprint(\"‚úÖ CELL 0 ready\")\nprint(\"RESULTS_DIR:\", RESULTS_DIR)",
      "outputs": []
    },
    {
      "id": "fb121720",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "# CELL 1 ‚Äî Runtime sanity check\n\nimport cv2, sklearn, torch, torchvision\n\nprint(\"Python:\", sys.version)\nprint(\"NumPy:\", np.__version__)\nprint(\"OpenCV:\", cv2.__version__)\nprint(\"scikit-learn:\", sklearn.__version__)\nprint(\"Torch:\", torch.__version__)\nprint(\"Torchvision:\", torchvision.__version__)\nprint(\"CUDA available:\", torch.cuda.is_available())\nprint(\"Device:\", \"cuda\" if torch.cuda.is_available() else \"cpu\")",
      "outputs": []
    },
    {
      "id": "a8e2e937",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "# CELL 2 ‚Äî Upload AT&T/ORL Faces ZIP and extract (auto-detect root)\n\nfrom google.colab import files\n\nDATA_PARENT = Path(\"/content/data\")\nDATA_PARENT.mkdir(parents=True, exist_ok=True)\n\nprint(\"Upload ORL/AT&T faces ZIP (must contain s1..s40 folders somewhere inside).\")\nuploaded = files.upload()\n\nzip_files = [n for n in uploaded.keys() if n.lower().endswith(\".zip\")]\nassert len(zip_files) > 0, \"‚ùå No ZIP uploaded. Upload the ORL/AT&T ZIP.\"\n\nzip_path = Path(\"/content\") / zip_files[0]\nextract_to = DATA_PARENT / \"att_extracted\"\nif extract_to.exists():\n    shutil.rmtree(extract_to)\nextract_to.mkdir(parents=True, exist_ok=True)\n\nwith zipfile.ZipFile(zip_path, \"r\") as z:\n    z.extractall(extract_to)\n\nDATA_ROOT = find_att_root(extract_to)\nassert DATA_ROOT is not None, \"‚ùå Could not find s1..s40 with .pgm inside the uploaded ZIP.\"\n\nprint(\"‚úÖ Detected DATA_ROOT:\", DATA_ROOT)\nprint(\"Example folders:\", sorted([p.name for p in DATA_ROOT.iterdir() if p.is_dir()])[:12])",
      "outputs": []
    },
    {
      "id": "575dd616",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "# CELL 3 ‚Äî Load dataset (memory-safe), show samples\n\nimport cv2\n\ndef load_att_faces(root: Path, hist_eq: bool = True):\n    images, labels, paths = [], [], []\n    subject_dirs = sorted(\n        [p for p in root.iterdir() if p.is_dir() and p.name.startswith(\"s\")],\n        key=lambda p: int(p.name[1:])\n    )\n    for s_idx, sdir in enumerate(subject_dirs):\n        for f in sorted(list(sdir.glob(\"*.pgm\"))):\n            img = cv2.imread(str(f), cv2.IMREAD_GRAYSCALE)\n            if img is None:\n                continue\n            if hist_eq:\n                img = cv2.equalizeHist(img)\n            images.append(img.astype(np.uint8))\n            labels.append(s_idx)\n            paths.append(str(f))\n    X = np.stack(images, axis=0)         # (N,H,W)\n    y = np.array(labels, dtype=np.int64) # (N,)\n    return X, y, paths\n\nX_img, y, img_paths = load_att_faces(DATA_ROOT, hist_eq=APPLY_HIST_EQ)\nn_classes = len(np.unique(y))\n\nprint(\"Loaded images:\", X_img.shape)\nprint(\"Unique subjects:\", n_classes)\nprint(\"Image shape:\", X_img[0].shape, \"dtype:\", X_img.dtype)\n\nassert n_classes == 40, \"Expected 40 subjects (s1..s40).\"\n\nshow_grid(X_img, y, n=20, cols=10, seed=1)",
      "outputs": []
    },
    {
      "id": "397e95e5",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "# CELL 4 ‚Äî Identity-stratified split (same split reused for both pipelines)\n\nSPLITS = {}\nfor s in SEEDS:\n    tr, te = split_per_identity(y, seed=s, n_test_per_subject=N_TEST_PER_SUBJECT)\n    SPLITS[s] = {\"train_idx\": tr, \"test_idx\": te}\n\nprint(\"Example split (seed=42):\")\nprint(\"Train size:\", len(SPLITS[42][\"train_idx\"]), \"Test size:\", len(SPLITS[42][\"test_idx\"]))\nassert len(SPLITS[42][\"test_idx\"]) == 40 * N_TEST_PER_SUBJECT",
      "outputs": []
    },
    {
      "id": "36e5ab70",
      "cell_type": "markdown",
      "source": "## Pipeline A ‚Äî PCA (Eigenfaces-style) + Linear SVM",
      "metadata": {}
    },
    {
      "id": "2080b581",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "# CELL 5 ‚Äî Preprocess for PCA\n\nfrom sklearn.preprocessing import FunctionTransformer, StandardScaler\n\nX_flat = X_img.reshape(len(X_img), -1)  # (N, 112*92=10304)\nto_float01 = FunctionTransformer(lambda Z: (Z.astype(np.float32) / 255.0), feature_names_out=\"one-to-one\")\ncenter_only = StandardScaler(with_std=False)\n\nprint(\"Flattened:\", X_flat.shape)",
      "outputs": []
    },
    {
      "id": "e4ca0322",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "# CELL 6 ‚Äî Tune PCA components + SVM + plot PCA-components curve (seed=42)\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold\n\ndef build_pipeline_a(seed: int):\n    return Pipeline([\n        (\"to_float01\", to_float01),\n        (\"center\", center_only),\n        (\"pca\", PCA(whiten=True, svd_solver=\"randomized\", random_state=seed)),\n        (\"svm\", SVC(kernel=\"linear\"))\n    ])\n\ndef tune_pipeline_a(X, y, train_idx, seed: int):\n    Xtr, ytr = X[train_idx], y[train_idx]\n    pipe = build_pipeline_a(seed)\n\n    param_grid = {\n        \"pca__n_components\": PCA_COMPONENTS_GRID,\n        \"svm__C\": SVM_C_GRID\n    }\n\n    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n    gs = GridSearchCV(pipe, param_grid=param_grid, scoring=\"accuracy\", cv=cv, n_jobs=-1, verbose=0)\n    gs.fit(Xtr, ytr)\n    return gs\n\n# CV curve (seed=42, fixed C=1)\nseed_curve = 42\ntr_curve = SPLITS[seed_curve][\"train_idx\"]\nXtr_curve, ytr_curve = X_flat[tr_curve], y[tr_curve]\n\ncurve_scores = []\nfor k in PCA_COMPONENTS_GRID:\n    pipe = build_pipeline_a(seed_curve)\n    pipe.set_params(pca__n_components=k, svm__C=1)\n\n    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed_curve)\n    gs = GridSearchCV(pipe, param_grid={\"pca__n_components\":[k], \"svm__C\":[1]}, scoring=\"accuracy\", cv=cv, n_jobs=-1, verbose=0)\n    gs.fit(Xtr_curve, ytr_curve)\n    curve_scores.append(gs.best_score_)\n\nplt.figure(figsize=(6,4))\nplt.plot(PCA_COMPONENTS_GRID, curve_scores, marker=\"o\")\nplt.xlabel(\"PCA components\")\nplt.ylabel(\"CV accuracy (seed=42, C=1)\")\nplt.title(\"Pipeline A: PCA components vs CV accuracy\")\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig(RESULTS_DIR / \"pca_components_curve_seed42.png\", dpi=200)\nplt.show()\n\nprint(\"Saved:\", RESULTS_DIR / \"pca_components_curve_seed42.png\")",
      "outputs": []
    },
    {
      "id": "16c06abe",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "# CELL 7 ‚Äî Train + evaluate Pipeline A (all seeds) + confusion matrix (seed=42)\n\nPIPE_A_RUNS = []\nPIPE_A_CM_SEED42 = None\n\nfor seed in SEEDS:\n    tr = SPLITS[seed][\"train_idx\"]\n    te = SPLITS[seed][\"test_idx\"]\n\n    gs = tune_pipeline_a(X_flat, y, tr, seed=seed)\n    model = gs.best_estimator_\n\n    Xtr, ytr = X_flat[tr], y[tr]\n    Xte, yte = X_flat[te], y[te]\n\n    t0 = time.perf_counter()\n    model.fit(Xtr, ytr)\n    train_s = time.perf_counter() - t0\n\n    t1 = time.perf_counter()\n    yhat = model.predict(Xte)\n    infer_s = time.perf_counter() - t1\n\n    acc, p, r, f1 = eval_metrics(yte, yhat)\n\n    PIPE_A_RUNS.append({\n        \"pipeline\": \"A_PCA+LinearSVM\",\n        \"seed\": seed,\n        \"best_params\": gs.best_params_,\n        \"acc\": float(acc),\n        \"macro_precision\": float(p),\n        \"macro_recall\": float(r),\n        \"macro_f1\": float(f1),\n        \"train_s\": float(train_s),\n        \"infer_ms_per_img\": float((infer_s / len(Xte)) * 1000.0),\n    })\n\n    if seed == 42:\n        PIPE_A_CM_SEED42 = save_confusion(\n            y_true=yte, y_pred=yhat,\n            title=\"Pipeline A ‚Äî PCA + Linear SVM (seed=42)\",\n            outpath=RESULTS_DIR / \"cm_pipelineA_seed42.png\",\n            n_classes=n_classes\n        )\n\nprint(\"‚úÖ Pipeline A runs:\", len(PIPE_A_RUNS))",
      "outputs": []
    },
    {
      "id": "ad9f0666",
      "cell_type": "markdown",
      "source": "## Pipeline B ‚Äî Transfer Learning Embeddings (ResNet18) + Linear SVM",
      "metadata": {}
    },
    {
      "id": "772d4284",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "# CELL 8 ‚Äî Load pretrained embedding model (ResNet18, feature extractor only)\n\nimport torch\nimport torchvision\nimport torch.nn as nn\nfrom torchvision.models import resnet18, ResNet18_Weights\nfrom PIL import Image\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nweights = ResNet18_Weights.DEFAULT\n\nbase = resnet18(weights=weights)\nembedder = nn.Sequential(*list(base.children())[:-1]).eval().to(device)  # outputs (B,512,1,1)\npreprocess = weights.transforms()\n\ndef gray_to_rgb_pil(gray_uint8: np.ndarray) -> Image.Image:\n    return Image.fromarray(gray_uint8).convert(\"RGB\")\n\nprint(\"‚úÖ Embedding model: ResNet18 (ImageNet pretrained)\")\nprint(\"Device:\", device)",
      "outputs": []
    },
    {
      "id": "44590bb6",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "# CELL 9 ‚Äî Compute embeddings (batched, memory-safe)\n\ndef compute_resnet_embeddings(X_gray_uint8: np.ndarray, batch_size: int = 64):\n    embs = []\n    with torch.no_grad():\n        for start in range(0, len(X_gray_uint8), batch_size):\n            batch = X_gray_uint8[start:start+batch_size]\n            batch_t = torch.stack([preprocess(gray_to_rgb_pil(im)) for im in batch], dim=0).to(device)\n\n            feat = embedder(batch_t)            # (B,512,1,1)\n            feat = feat.view(feat.size(0), -1)  # (B,512)\n            feat = torch.nn.functional.normalize(feat, p=2, dim=1)\n            embs.append(feat.cpu().numpy().astype(np.float32))\n    return np.vstack(embs)\n\nt0 = time.perf_counter()\nX_emb = compute_resnet_embeddings(X_img, batch_size=EMB_BATCH_SIZE)\nt1 = time.perf_counter()\n\nprint(\"Embeddings:\", X_emb.shape, X_emb.dtype)\nprint(f\"Embedding extraction: {t1-t0:.2f}s total, {(t1-t0)/len(X_emb)*1000:.2f} ms/img\")",
      "outputs": []
    },
    {
      "id": "e5343320",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "# CELL 10 ‚Äî Tune + train + evaluate Pipeline B (all seeds) + confusion matrix (seed=42)\n\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\ndef tune_pipeline_b(Xemb, y, train_idx, seed: int):\n    Xtr, ytr = Xemb[train_idx], y[train_idx]\n    pipe = Pipeline([\n        (\"scaler\", StandardScaler()),\n        (\"svm\", SVC(kernel=\"linear\"))\n    ])\n    param_grid = {\"svm__C\": SVM_C_GRID}\n    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n    gs = GridSearchCV(pipe, param_grid=param_grid, scoring=\"accuracy\", cv=cv, n_jobs=-1, verbose=0)\n    gs.fit(Xtr, ytr)\n    return gs\n\nPIPE_B_RUNS = []\nPIPE_B_CM_SEED42 = None\n\nfor seed in SEEDS:\n    tr = SPLITS[seed][\"train_idx\"]\n    te = SPLITS[seed][\"test_idx\"]\n\n    gs = tune_pipeline_b(X_emb, y, tr, seed=seed)\n    model = gs.best_estimator_\n\n    Xtr, ytr = X_emb[tr], y[tr]\n    Xte, yte = X_emb[te], y[te]\n\n    t0 = time.perf_counter()\n    model.fit(Xtr, ytr)\n    train_s = time.perf_counter() - t0\n\n    t1 = time.perf_counter()\n    yhat = model.predict(Xte)\n    infer_s = time.perf_counter() - t1\n\n    acc, p, r, f1 = eval_metrics(yte, yhat)\n\n    PIPE_B_RUNS.append({\n        \"pipeline\": \"B_ResNet18Embeddings+LinearSVM\",\n        \"seed\": seed,\n        \"best_params\": gs.best_params_,\n        \"acc\": float(acc),\n        \"macro_precision\": float(p),\n        \"macro_recall\": float(r),\n        \"macro_f1\": float(f1),\n        \"train_s\": float(train_s),\n        \"infer_ms_per_img\": float((infer_s / len(Xte)) * 1000.0),\n    })\n\n    if seed == 42:\n        PIPE_B_CM_SEED42 = save_confusion(\n            y_true=yte, y_pred=yhat,\n            title=\"Pipeline B ‚Äî ResNet18 embeddings + Linear SVM (seed=42)\",\n            outpath=RESULTS_DIR / \"cm_pipelineB_seed42.png\",\n            n_classes=n_classes\n        )\n\nprint(\"‚úÖ Pipeline B runs:\", len(PIPE_B_RUNS))",
      "outputs": []
    },
    {
      "id": "c2f0cb3e",
      "cell_type": "markdown",
      "source": "## Comparison ‚Äî tables + extra graphs",
      "metadata": {}
    },
    {
      "id": "e1ea2ee3",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "# CELL 11 ‚Äî Build comparison tables + save CSVs\n\nimport pandas as pd\n\ndf_a = pd.DataFrame(PIPE_A_RUNS)\ndf_b = pd.DataFrame(PIPE_B_RUNS)\ndf = pd.concat([df_a, df_b], ignore_index=True)\n\ndisplay(df[[\"pipeline\",\"seed\",\"acc\",\"macro_f1\",\"train_s\",\"infer_ms_per_img\",\"best_params\"]])\n\nsummary = df.groupby(\"pipeline\").agg(\n    acc_mean=(\"acc\",\"mean\"),\n    acc_std=(\"acc\",\"std\"),\n    f1_mean=(\"macro_f1\",\"mean\"),\n    f1_std=(\"macro_f1\",\"std\"),\n    train_s_mean=(\"train_s\",\"mean\"),\n    train_s_std=(\"train_s\",\"std\"),\n    infer_ms_mean=(\"infer_ms_per_img\",\"mean\"),\n    infer_ms_std=(\"infer_ms_per_img\",\"std\"),\n).reset_index()\n\ndisplay(summary)\n\ndf.to_csv(RESULTS_DIR / \"runs_all_seeds.csv\", index=False)\nsummary.to_csv(RESULTS_DIR / \"summary_mean_std.csv\", index=False)\nprint(\"Saved CSVs to:\", RESULTS_DIR)",
      "outputs": []
    },
    {
      "id": "64a08a35",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "# CELL 12 ‚Äî Extra comparison graphs (per-seed bars + boxplots)\n\nseed_order = SEEDS\n\ndef plot_seed_bars(metric, title, ylabel, outname):\n    pivot = df.pivot(index=\"seed\", columns=\"pipeline\", values=metric).loc[seed_order]\n    ax = pivot.plot(kind=\"bar\", figsize=(9,4))\n    ax.set_title(title)\n    ax.set_xlabel(\"Seed\")\n    ax.set_ylabel(ylabel)\n    ax.grid(True, axis=\"y\", alpha=0.3)\n    plt.tight_layout()\n    plt.savefig(RESULTS_DIR / outname, dpi=200)\n    plt.show()\n    print(\"Saved:\", RESULTS_DIR / outname)\n\nplot_seed_bars(\"acc\", \"Accuracy by seed\", \"Accuracy\", \"acc_by_seed.png\")\nplot_seed_bars(\"macro_f1\", \"Macro F1 by seed\", \"Macro F1\", \"f1_by_seed.png\")\nplot_seed_bars(\"train_s\", \"Training time by seed\", \"Seconds\", \"train_time_by_seed.png\")\nplot_seed_bars(\"infer_ms_per_img\", \"Inference time by seed\", \"ms / image\", \"infer_time_by_seed.png\")\n\nplt.figure(figsize=(8,4))\ndf.boxplot(column=\"acc\", by=\"pipeline\")\nplt.title(\"Accuracy distribution across seeds\")\nplt.suptitle(\"\")\nplt.ylabel(\"Accuracy\")\nplt.grid(True, axis=\"y\", alpha=0.3)\nplt.tight_layout()\nplt.savefig(RESULTS_DIR / \"acc_boxplot.png\", dpi=200)\nplt.show()\nprint(\"Saved:\", RESULTS_DIR / \"acc_boxplot.png\")\n\nplt.figure(figsize=(8,4))\ndf.boxplot(column=\"macro_f1\", by=\"pipeline\")\nplt.title(\"Macro F1 distribution across seeds\")\nplt.suptitle(\"\")\nplt.ylabel(\"Macro F1\")\nplt.grid(True, axis=\"y\", alpha=0.3)\nplt.tight_layout()\nplt.savefig(RESULTS_DIR / \"f1_boxplot.png\", dpi=200)\nplt.show()\nprint(\"Saved:\", RESULTS_DIR / \"f1_boxplot.png\")",
      "outputs": []
    },
    {
      "id": "6255ce94",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "# CELL 13 ‚Äî Per-class accuracy plot (seed=42) for both pipelines\n\nassert PIPE_A_CM_SEED42 is not None and PIPE_B_CM_SEED42 is not None\n\naccA = per_class_accuracy(PIPE_A_CM_SEED42)\naccB = per_class_accuracy(PIPE_B_CM_SEED42)\n\nx = np.arange(len(accA))\nplt.figure(figsize=(12,4))\nplt.plot(x, accA, marker=\"o\", linewidth=1, label=\"Pipeline A\")\nplt.plot(x, accB, marker=\"o\", linewidth=1, label=\"Pipeline B\")\nplt.ylim(0, 1.05)\nplt.xlabel(\"Class (subject index 0..39)\")\nplt.ylabel(\"Per-class accuracy (seed=42)\")\nplt.title(\"Per-class accuracy comparison (seed=42)\")\nplt.grid(True, alpha=0.3)\nplt.legend()\nplt.tight_layout()\nplt.savefig(RESULTS_DIR / \"per_class_accuracy_seed42.png\", dpi=200)\nplt.show()\n\nprint(\"Saved:\", RESULTS_DIR / \"per_class_accuracy_seed42.png\")",
      "outputs": []
    },
    {
      "id": "6e25ed9b",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "# CELL 14 ‚Äî Winner decision\n\nbest = summary.sort_values([\"f1_mean\",\"acc_mean\"], ascending=False).iloc[0]\nprint(\"üèÜ Winner:\", best[\"pipeline\"])\nprint(best)\n\nprint(\"\\nConclusion:\")\nprint(f\"- Recommended deployment pipeline: {best['pipeline']}\")\nprint(\"- Pipeline A is the classical baseline (PCA/Eigenfaces-style).\")\nprint(\"- Pipeline B uses transfer learning embeddings (feature extractor) for robustness and higher accuracy.\")",
      "outputs": []
    },
    {
      "id": "8bb05254",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "# CELL 15 ‚Äî Zip results + download\n\nzip_path = Path(\"/content/results.zip\")\nif zip_path.exists():\n    zip_path.unlink()\n\nshutil.make_archive(\"/content/results\", \"zip\", RESULTS_DIR)\nprint(\"Created:\", zip_path)\n\nfrom google.colab import files\nfiles.download(str(zip_path))",
      "outputs": []
    },
    {
      "id": "214bc1e9",
      "cell_type": "markdown",
      "source": "## Optional: FaceNet embeddings (advanced, off by default)\n\nYour Colab runtime may use very new Torch/Torchvision versions; some FaceNet packages can break on these.\nThis optional cell is written so it **won‚Äôt crash** your notebook if FaceNet fails.\n\nIf FaceNet import fails, keep Pipeline B (ResNet18) ‚Äî it‚Äôs already very strong on ORL.\n",
      "metadata": {}
    },
    {
      "id": "7b55ee32",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "# OPTIONAL CELL ‚Äî FaceNet embeddings (run only if you want to experiment)\n\nUSE_FACENET = False\n\nif USE_FACENET:\n    try:\n        !pip -q install facenet-pytorch\n        from facenet_pytorch import InceptionResnetV1, fixed_image_standardization\n        import torchvision.transforms as T\n        from PIL import Image\n        import torch\n\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        facenet = InceptionResnetV1(pretrained=\"vggface2\").eval().to(device)\n\n        EMB_SIZE = 160\n        facenet_tf = T.Compose([\n            T.Resize((EMB_SIZE, EMB_SIZE)),\n            T.ToTensor(),\n            fixed_image_standardization\n        ])\n\n        def facenet_embed(X_gray_uint8: np.ndarray, batch_size=32):\n            embs = []\n            with torch.no_grad():\n                for start in range(0, len(X_gray_uint8), batch_size):\n                    batch = X_gray_uint8[start:start+batch_size]\n                    bt = torch.stack([facenet_tf(Image.fromarray(im).convert(\"RGB\")) for im in batch], dim=0).to(device)\n                    out = facenet(bt)\n                    out = torch.nn.functional.normalize(out, p=2, dim=1)\n                    embs.append(out.cpu().numpy().astype(np.float32))\n            return np.vstack(embs)\n\n        X_emb_facenet = facenet_embed(X_img, batch_size=32)\n        print(\"‚úÖ FaceNet embeddings:\", X_emb_facenet.shape)\n        print(\"Next: reuse Pipeline B SVM code with X_emb_facenet instead of X_emb.\")\n    except Exception as e:\n        print(\"‚ùå FaceNet failed on this runtime. Keep the stable ResNet18 pipeline.\")\n        print(\"Error:\", repr(e))",
      "outputs": []
    }
  ]
}